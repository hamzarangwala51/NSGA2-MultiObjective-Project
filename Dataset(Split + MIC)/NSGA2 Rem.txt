The Non-dominated Sorting Genetic Algorithm II (NSGA-II) is a widely used evolutionary algorithm designed for solving multi-objective optimization problems. NSGA-II improves upon its predecessor, NSGA, by addressing key issues such as computational complexity, lack of elitism, and the need for specifying a sharing parameter. NSGA-II outperforms two other contemporary MOEAs: Pareto-archived evolution strategy (PAES) and strength Pareto EA (SPEA) in terms of finding a diverse set of solutions and in converging near the true Pareto-optimal set [12].
The MOO problem has one of the classification methods as the Pareto method, it is used if the desired solutions and performance indicators are separate and produce a compromise solution (trade-off) and can be displayed in the form of Pareto optimal front (POF) [21]. NSGA-II operates by simulating the process of natural evolution. The algorithm maintains a population of potential solutions that evolve over successive generations. Each solution in the population is evaluated based on multiple objective functions, which, in this study, are the minimization of feature count and classification error.

In optimisation problems, the location of the global optimum solution is unknown a priori, and initialisation is a stochastic process. The initialisation control parameters of population-based metaheuristic algorithms play a significant role in improving the performance of the algorithms [22]. During the initialization of the algorithm the population needs to be diverse, it will ensure that the initial population covers the overall solution space and converges to an optimal set of solutions. This is being reiterated multiple times to suffice the number of individuals required to accommodate the initial population size. Generating the initial population is one of the important steps in evolutionary algorithms. A poor initial population may unnecessarily increase the number of searches, or it may cause the algorithm to converge at local optima [23]. 



Similar to other population-based evolutionary algorithms, a set of uniform random individuals are produced as an initial population. Feature selection is originally a high dimensional binary optimization problem. Each individual in the population is a vector in the length of the number of all features. The variable in the vector indicate the selection status of the features by using a binary value (0 or 1). The value of one indicates the selection of corresponding feature and value of zero means that the feature is not selected. In order to compute the error of classification on each subset (each solution in the population), features associated with value of one are selected to form the dataset .[24]

